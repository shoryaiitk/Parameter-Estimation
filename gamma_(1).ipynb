{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th_3lHqkKK9D",
        "outputId": "22ef8a29-38ce-44e7-8ee0-c37f42d20fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Training and test data saved to /content/drive/MyDrive/deep_learning/data_large.npz\n",
            "Loaded weights from /content/drive/MyDrive/deep_learning/model_checkpoint.h5\n",
            "Epoch 1/1000\n",
            "125000/125000 [==============================] - 804s 6ms/step - loss: 0.7466 - shape_output_loss: 0.3208 - scale_output_loss: 0.1622 - shape_output_mean_squared_error: 0.3208 - scale_output_mean_squared_error: 0.1622 - val_loss: 0.7274 - val_shape_output_loss: 0.3127 - val_scale_output_loss: 0.1582 - val_shape_output_mean_squared_error: 0.3127 - val_scale_output_mean_squared_error: 0.1582\n",
            "Epoch 2/1000\n",
            "125000/125000 [==============================] - 749s 6ms/step - loss: 0.7385 - shape_output_loss: 0.3179 - scale_output_loss: 0.1595 - shape_output_mean_squared_error: 0.3179 - scale_output_mean_squared_error: 0.1595 - val_loss: 0.7313 - val_shape_output_loss: 0.3104 - val_scale_output_loss: 0.1581 - val_shape_output_mean_squared_error: 0.3104 - val_scale_output_mean_squared_error: 0.1581\n",
            "Epoch 3/1000\n",
            "125000/125000 [==============================] - 764s 6ms/step - loss: 0.7386 - shape_output_loss: 0.3179 - scale_output_loss: 0.1594 - shape_output_mean_squared_error: 0.3179 - scale_output_mean_squared_error: 0.1594 - val_loss: 0.7278 - val_shape_output_loss: 0.3073 - val_scale_output_loss: 0.1599 - val_shape_output_mean_squared_error: 0.3073 - val_scale_output_mean_squared_error: 0.1599\n",
            "Epoch 4/1000\n",
            "125000/125000 [==============================] - 697s 6ms/step - loss: 0.7382 - shape_output_loss: 0.3175 - scale_output_loss: 0.1594 - shape_output_mean_squared_error: 0.3175 - scale_output_mean_squared_error: 0.1594 - val_loss: 0.7282 - val_shape_output_loss: 0.3084 - val_scale_output_loss: 0.1593 - val_shape_output_mean_squared_error: 0.3084 - val_scale_output_mean_squared_error: 0.1593\n",
            "Epoch 5/1000\n",
            "125000/125000 [==============================] - 713s 6ms/step - loss: 0.7381 - shape_output_loss: 0.3170 - scale_output_loss: 0.1593 - shape_output_mean_squared_error: 0.3170 - scale_output_mean_squared_error: 0.1593 - val_loss: 0.7329 - val_shape_output_loss: 0.3132 - val_scale_output_loss: 0.1589 - val_shape_output_mean_squared_error: 0.3132 - val_scale_output_mean_squared_error: 0.1589\n",
            "Epoch 6/1000\n",
            "125000/125000 [==============================] - 709s 6ms/step - loss: 0.7379 - shape_output_loss: 0.3167 - scale_output_loss: 0.1594 - shape_output_mean_squared_error: 0.3167 - scale_output_mean_squared_error: 0.1594 - val_loss: 0.7399 - val_shape_output_loss: 0.3176 - val_scale_output_loss: 0.1592 - val_shape_output_mean_squared_error: 0.3176 - val_scale_output_mean_squared_error: 0.1592\n",
            "31250/31250 [==============================] - 72s 2ms/step - loss: 0.7274 - shape_output_loss: 0.3127 - scale_output_loss: 0.1582 - shape_output_mean_squared_error: 0.3127 - scale_output_mean_squared_error: 0.1582\n",
            "Test Loss: 0.7274158000946045, Test MSE for Shape: 0.31269320845603943, Test MSE for Scale: 0.15815266966819763\n",
            "31250/31250 [==============================] - 66s 2ms/step\n",
            "Sample predictions:\n",
            "Predicted Shape (a): 3.917369842529297, Actual Shape (a): 3.2224353985036536\n",
            "Predicted Scale (b): 1.317670226097107, Actual Scale (b): 1.9202425838560648\n",
            "Predicted Shape (a): 2.4489521980285645, Actual Shape (a): 1.8504356730328726\n",
            "Predicted Scale (b): 1.063153862953186, Actual Scale (b): 1.1780305749748319\n",
            "Predicted Shape (a): 3.2914857864379883, Actual Shape (a): 3.203387826366785\n",
            "Predicted Scale (b): 1.1422967910766602, Actual Scale (b): 1.1152372946692333\n",
            "Predicted Shape (a): 4.118392467498779, Actual Shape (a): 4.978473949834745\n",
            "Predicted Scale (b): 1.4059603214263916, Actual Scale (b): 0.7607436734789349\n",
            "Predicted Shape (a): 3.9807138442993164, Actual Shape (a): 4.9517190441527745\n",
            "Predicted Scale (b): 1.343747854232788, Actual Scale (b): 0.9531691039956575\n",
            "\n",
            "Comparison with MLE:\n",
            "Sample 1: MLE Shape: 3.4461, Predicted Shape: 3.9174\n",
            "Sample 1: MLE Scale: 1.5232, Predicted Scale: 1.3177\n",
            "Sample 2: MLE Shape: 2.2472, Predicted Shape: 2.4490\n",
            "Sample 2: MLE Scale: 0.9370, Predicted Scale: 1.0632\n",
            "Sample 3: MLE Shape: 3.4324, Predicted Shape: 3.2915\n",
            "Sample 3: MLE Scale: 0.9677, Predicted Scale: 1.1423\n",
            "Sample 4: MLE Shape: 7.3813, Predicted Shape: 4.1184\n",
            "Sample 4: MLE Scale: 0.5491, Predicted Scale: 1.4060\n",
            "Sample 5: MLE Shape: 4.9841, Predicted Shape: 3.9807\n",
            "Sample 5: MLE Scale: 0.9426, Predicted Scale: 1.3437\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, ELU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import gamma\n",
        "import os\n",
        "\n",
        "np.random.seed(60)\n",
        "\n",
        "# Function to generate synthetic data for Gamma distribution parameters (shape and scale)\n",
        "def generate_gamma_data(num_shapes=200, num_scales=200):\n",
        "    a_values = np.random.uniform(1, 5, num_shapes)\n",
        "    b_values = np.random.uniform(0.5, 2, num_scales)\n",
        "\n",
        "    x = []\n",
        "    y_shape = []\n",
        "    y_scale = []\n",
        "    sample_sizes = np.random.randint(10, 100, 25)\n",
        "\n",
        "    for a in a_values:\n",
        "        for b in b_values:\n",
        "            for num_samples in sample_sizes:\n",
        "                for _ in range(5):\n",
        "                    samples = np.random.gamma(shape=a, scale=b, size=num_samples)\n",
        "                    samples = np.sort(samples)\n",
        "                    x.append(np.insert(samples, 0, num_samples))\n",
        "                    y_shape.append([a])\n",
        "                    y_scale.append([b])\n",
        "\n",
        "    max_length = 101\n",
        "    x = pad_sequences(x, maxlen=max_length, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "    x = np.array(x)\n",
        "    y_shape = np.array(y_shape)\n",
        "    y_scale = np.array(y_scale)\n",
        "\n",
        "    return x, y_shape, y_scale\n",
        "\n",
        "# Generate synthetic data for Gamma distribution\n",
        "x, y_shape, y_scale = generate_gamma_data()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train_shape, y_test_shape, y_train_scale, y_test_scale = train_test_split(\n",
        "    x, y_shape, y_scale, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Save data to Google Drive (adjust the path as needed)\n",
        "data_dir = \"/content/drive/MyDrive/deep_learning\"\n",
        "data_path = os.path.join(data_dir, \"data_large.npz\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "np.savez(data_path,\n",
        "         x_train=x_train,\n",
        "         x_test=x_test,\n",
        "         y_train_shape=y_train_shape,\n",
        "         y_train_scale=y_train_scale,\n",
        "         y_test_shape=y_test_shape,\n",
        "         y_test_scale=y_test_scale)\n",
        "print(f\"Training and test data saved to {data_path}\")\n",
        "\n",
        "# Define the neural network architecture\n",
        "input_layer = Input(shape=(101,))\n",
        "hidden1 = Dense(101, kernel_regularizer=l1(0.01))(input_layer)\n",
        "hidden1 = ELU(alpha=1.0)(hidden1)\n",
        "hidden2 = Dense(101, kernel_regularizer=l2(0.01))(hidden1)\n",
        "hidden2 = LeakyReLU(alpha=0.01)(hidden2)\n",
        "hidden3 = Dense(101, kernel_regularizer=l1(0.01))(hidden2)\n",
        "hidden3 = ELU(alpha=1.0)(hidden3)\n",
        "hidden4 = Dense(101, kernel_regularizer=l2(0.01))(hidden3)\n",
        "hidden4 = LeakyReLU(alpha=0.01)(hidden4)\n",
        "\n",
        "# Branch for shape (a)\n",
        "shape_hidden5 = Dense(101, kernel_regularizer=l1(0.01))(hidden4)\n",
        "shape_hidden5 = ELU(alpha=1.0)(shape_hidden5)\n",
        "shape_hidden6 = Dense(101, kernel_regularizer=l2(0.01))(shape_hidden5)\n",
        "shape_hidden6 = LeakyReLU(alpha=0.01)(shape_hidden6)\n",
        "shape_hidden7 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden6)\n",
        "shape_hidden7 = ELU(alpha=1.0)(shape_hidden7)\n",
        "shape_hidden8 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden7)\n",
        "shape_hidden8 = LeakyReLU(alpha=0.01)(shape_hidden8)\n",
        "shape_hidden9 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden8)\n",
        "shape_hidden9 = ELU(alpha=1.0)(shape_hidden9)\n",
        "shape_hidden10 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden9)\n",
        "shape_hidden10 = LeakyReLU(alpha=0.01)(shape_hidden10)\n",
        "shape_hidden11 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden10)\n",
        "shape_hidden11 = ELU(alpha=1.0)(shape_hidden11)\n",
        "shape_hidden12 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden11)\n",
        "shape_hidden12 = LeakyReLU(alpha=0.01)(shape_hidden12)\n",
        "shape_hidden13 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden12)\n",
        "shape_hidden13 = ELU(alpha=1.0)(shape_hidden13)\n",
        "shape_hidden14 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden13)\n",
        "shape_hidden14 = LeakyReLU(alpha=0.01)(shape_hidden14)\n",
        "shape_output = Dense(1, activation='linear', kernel_regularizer=l2(0.01), name='shape_output')(shape_hidden14)\n",
        "\n",
        "# Branch for scale (b)\n",
        "scale_hidden5 = Dense(101, kernel_regularizer=l1(0.01))(hidden4)\n",
        "scale_hidden5 = ELU(alpha=1.0)(scale_hidden5)\n",
        "scale_hidden6 = Dense(101, kernel_regularizer=l2(0.01))(scale_hidden5)\n",
        "scale_hidden6 = LeakyReLU(alpha=0.01)(scale_hidden6)\n",
        "scale_hidden7 = Dense(10, kernel_regularizer=l1(0.01))(scale_hidden6)\n",
        "scale_hidden7 = ELU(alpha=1.0)(scale_hidden7)\n",
        "scale_hidden8 = Dense(10, kernel_regularizer=l2(0.01))(scale_hidden7)\n",
        "scale_hidden8 = LeakyReLU(alpha=0.01)(scale_hidden8)\n",
        "scale_output = Dense(1, activation='linear', kernel_regularizer=l2(0.01), name='scale_output')(scale_hidden8)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=[shape_output, scale_output])\n",
        "\n",
        "# Compile the model with separate losses for each output\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'shape_output': 'mean_squared_error', 'scale_output': 'mean_squared_error'},\n",
        "    metrics={'shape_output': ['mean_squared_error'], 'scale_output': ['mean_squared_error']}\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_path = os.path.join(data_dir, \"model_checkpoint.h5\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print(f\"Loaded weights from {checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"No checkpoint found at {checkpoint_path}. Training from scratch.\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, {'shape_output': y_train_shape, 'scale_output': y_train_scale},\n",
        "    validation_data=(x_test, {'shape_output': y_test_shape, 'scale_output': y_test_scale}),\n",
        "    epochs=1000, batch_size=32,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Save the final model weights\n",
        "model.save_weights(checkpoint_path)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation = model.evaluate(x_test, {'shape_output': y_test_shape, 'scale_output': y_test_scale})\n",
        "loss, mse_shape, mse_scale = evaluation[0], evaluation[1], evaluation[2]\n",
        "print(f\"Test Loss: {loss}, Test MSE for Shape: {mse_shape}, Test MSE for Scale: {mse_scale}\")\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(x_test)\n",
        "predicted_shapes = predictions[0]\n",
        "predicted_scales = predictions[1]\n",
        "\n",
        "print(\"Sample predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Predicted Shape (a): {predicted_shapes[i][0]}, Actual Shape (a): {y_test_shape[i][0]}\")\n",
        "    print(f\"Predicted Scale (b): {predicted_scales[i][0]}, Actual Scale (b): {y_test_scale[i][0]}\")\n",
        "\n",
        "# Compare with MLE\n",
        "def gamma_mle(samples):\n",
        "    shape, loc, scale = gamma.fit(samples, floc=0)\n",
        "    return shape, scale\n",
        "\n",
        "print(\"\\nComparison with MLE:\")\n",
        "for i in range(5):\n",
        "    sample = x_test[i][1:1 + int(x_test[i][0])]\n",
        "    mle_shape, mle_scale = gamma_mle(sample)\n",
        "    print(f\"Sample {i+1}: MLE Shape: {mle_shape:.4f}, Predicted Shape: {predicted_shapes[i][0]:.4f}\")\n",
        "    print(f\"Sample {i+1}: MLE Scale: {mle_scale:.4f}, Predicted Scale: {predicted_scales[i][0]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVSyp8ihky5s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}