{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th_3lHqkKK9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f88007-0b91-42a8-e163-fda1f577e08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, ELU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import gamma\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(70)\n",
        "\n",
        "# Function to generate synthetic data for Gamma distribution parameters (shape and scale)\n",
        "def generate_gamma_data(num_shapes=20, num_scales=20):\n",
        "    a_values = np.random.uniform(1, 5, num_shapes)\n",
        "    b_values = np.random.uniform(0.5, 2, num_scales)\n",
        "\n",
        "    x = []\n",
        "    y_shape = []\n",
        "    y_scale = []\n",
        "    sample_sizes = np.random.randint(10, 100, 25)\n",
        "\n",
        "    for a in a_values:\n",
        "        for b in b_values:\n",
        "            for num_samples in sample_sizes:\n",
        "                for _ in range(5):\n",
        "                    samples = np.random.gamma(shape=a, scale=b, size=num_samples)\n",
        "                    samples = np.sort(samples)\n",
        "                    x.append(np.insert(samples, 0, num_samples))\n",
        "                    y_shape.append([a])\n",
        "                    y_scale.append([b])\n",
        "\n",
        "    max_length = 101\n",
        "    x = pad_sequences(x, maxlen=max_length, dtype='float32', padding='post', truncating='post')\n",
        "\n",
        "    x = np.array(x)\n",
        "    y_shape = np.array(y_shape)\n",
        "    y_scale = np.array(y_scale)\n",
        "\n",
        "    return x, y_shape, y_scale\n",
        "\n",
        "# Generate synthetic data for Gamma distribution\n",
        "x, y_shape, y_scale = generate_gamma_data()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train_shape, y_test_shape, y_train_scale, y_test_scale = train_test_split(\n",
        "    x, y_shape, y_scale, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Save data to Google Drive (adjust the path as needed)\n",
        "data_dir = \"/content/drive/MyDrive/Parameter_estimation/gamma_distribution\"\n",
        "data_path = os.path.join(data_dir, \"data_large.npz\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "np.savez(data_path,\n",
        "         x_train=x_train,\n",
        "         x_test=x_test,\n",
        "         y_train_shape=y_train_shape,\n",
        "         y_train_scale=y_train_scale,\n",
        "         y_test_shape=y_test_shape,\n",
        "         y_test_scale=y_test_scale)\n",
        "print(f\"Training and test data saved to {data_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4K2naI0jNrdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0b9467-8a54-42b7-d35b-561aacaf13fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test data saved to /content/drive/MyDrive/Parameter_estimation/gamma_distribution/data_large.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"./drive/MyDrive/Parameter_estimation/gamma_distribution/data.npz\"\n",
        "# Function to load data from file\n",
        "def load_data(data_path):\n",
        "    with np.load(data_path) as data:\n",
        "        x_train = data['x_train']\n",
        "        x_test = data['x_test']\n",
        "        y_train_shape = data['y_train_shape']\n",
        "        y_train_scale = data['y_train_scale']\n",
        "        y_test_shape = data['y_test_shape']\n",
        "        y_test_scale = data['y_test_scale']\n",
        "        return x_train, x_test, y_train_shape, y_train_scale, y_test_shape, y_test_scale\n",
        "\n",
        "# Load data\n",
        "x_train, x_test, y_train_shape, y_train_scale, y_test_shape, y_test_scale = load_data(data_path)"
      ],
      "metadata": {
        "id": "GfQvTF-jO-Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "input_layer = Input(shape=(101,))\n",
        "hidden1 = Dense(101, kernel_regularizer=l1(0.01))(input_layer)\n",
        "hidden1 = ELU(alpha=1.0)(hidden1)\n",
        "hidden2 = Dense(101, kernel_regularizer=l2(0.01))(hidden1)\n",
        "hidden2 = LeakyReLU(alpha=0.01)(hidden2)\n",
        "hidden3 = Dense(101, kernel_regularizer=l1(0.01))(hidden2)\n",
        "hidden3 = ELU(alpha=1.0)(hidden3)\n",
        "hidden4 = Dense(101, kernel_regularizer=l2(0.01))(hidden3)\n",
        "hidden4 = LeakyReLU(alpha=0.01)(hidden4)\n",
        "\n",
        "# Branch for shape (a)\n",
        "shape_hidden5 = Dense(101, kernel_regularizer=l1(0.01))(hidden4)\n",
        "shape_hidden5 = ELU(alpha=1.0)(shape_hidden5)\n",
        "shape_hidden6 = Dense(101, kernel_regularizer=l2(0.01))(shape_hidden5)\n",
        "shape_hidden6 = LeakyReLU(alpha=0.01)(shape_hidden6)\n",
        "shape_hidden7 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden6)\n",
        "shape_hidden7 = ELU(alpha=1.0)(shape_hidden7)\n",
        "shape_hidden8 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden7)\n",
        "shape_hidden8 = LeakyReLU(alpha=0.01)(shape_hidden8)\n",
        "shape_hidden9 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden8)\n",
        "shape_hidden9 = ELU(alpha=1.0)(shape_hidden9)\n",
        "shape_hidden10 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden9)\n",
        "shape_hidden10 = LeakyReLU(alpha=0.01)(shape_hidden10)\n",
        "shape_hidden11 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden10)\n",
        "shape_hidden11 = ELU(alpha=1.0)(shape_hidden11)\n",
        "shape_hidden12 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden11)\n",
        "shape_hidden12 = LeakyReLU(alpha=0.01)(shape_hidden12)\n",
        "shape_hidden13 = Dense(10, kernel_regularizer=l1(0.01))(shape_hidden12)\n",
        "shape_hidden13 = ELU(alpha=1.0)(shape_hidden13)\n",
        "shape_hidden14 = Dense(10, kernel_regularizer=l2(0.01))(shape_hidden13)\n",
        "shape_hidden14 = LeakyReLU(alpha=0.01)(shape_hidden14)\n",
        "shape_output = Dense(1, activation='linear', kernel_regularizer=l2(0.01), name='shape_output')(shape_hidden14)\n",
        "\n",
        "# Branch for scale (b)\n",
        "scale_hidden5 = Dense(101, kernel_regularizer=l1(0.01))(hidden4)\n",
        "scale_hidden5 = ELU(alpha=1.0)(scale_hidden5)\n",
        "scale_hidden6 = Dense(101, kernel_regularizer=l2(0.01))(scale_hidden5)\n",
        "scale_hidden6 = LeakyReLU(alpha=0.01)(scale_hidden6)\n",
        "scale_hidden7 = Dense(10, kernel_regularizer=l1(0.01))(scale_hidden6)\n",
        "scale_hidden7 = ELU(alpha=1.0)(scale_hidden7)\n",
        "scale_hidden8 = Dense(10, kernel_regularizer=l2(0.01))(scale_hidden7)\n",
        "scale_hidden8 = LeakyReLU(alpha=0.01)(scale_hidden8)\n",
        "scale_output = Dense(1, activation='linear', kernel_regularizer=l2(0.01), name='scale_output')(scale_hidden8)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=[shape_output, scale_output])\n",
        "\n",
        "# Compile the model with separate losses for each output\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={'shape_output': 'mean_squared_error', 'scale_output': 'mean_squared_error'},\n",
        "    metrics={'shape_output': ['mean_squared_error'], 'scale_output': ['mean_squared_error']}\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "data_dir = \"/content/drive/MyDrive/Parameter_estimation/gamma_distribution\"\n",
        "checkpoint_path = os.path.join(data_dir, \"model_checkpoint.h5\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq='epoch'\n",
        ")\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print(f\"Loaded weights from {checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"No checkpoint found at {checkpoint_path}. Training from scratch.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Q7ZuwdORQo",
        "outputId": "b1533e61-e374-4cf4-fc68-4dc00ae41eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights from /content/drive/MyDrive/Parameter_estimation/gamma_distribution/model_checkpoint.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, {'shape_output': y_train_shape, 'scale_output': y_train_scale},\n",
        "    validation_data=(x_test, {'shape_output': y_test_shape, 'scale_output': y_test_scale}),\n",
        "    epochs=1000, batch_size=32,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Save the final model weights\n",
        "model.save_weights(checkpoint_path)\n",
        "print(f\"Model weights saved to {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "ugw6nWeyOgbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "evaluation = model.evaluate(x_test, {'shape_output': y_test_shape, 'scale_output': y_test_scale})\n",
        "loss, mse_shape, mse_scale = evaluation[0], evaluation[1], evaluation[2]\n",
        "print(f\"Test Loss: {loss}, Test MSE for Shape: {mse_shape}, Test MSE for Scale: {mse_scale}\")\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(x_test)\n",
        "predicted_shapes = predictions[0]\n",
        "predicted_scales = predictions[1]\n",
        "\n",
        "print(\"Sample predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Predicted Shape (a): {predicted_shapes[i][0]}, Actual Shape (a): {y_test_shape[i][0]}\")\n",
        "    print(f\"Predicted Scale (b): {predicted_scales[i][0]}, Actual Scale (b): {y_test_scale[i][0]}\")\n",
        "\n",
        "# Compare with MLE\n",
        "def gamma_mle(samples):\n",
        "    shape, loc, scale = gamma.fit(samples, floc=0)\n",
        "    return shape, scale\n",
        "\n",
        "print(\"\\nComparison with MLE:\")\n",
        "for i in range(5):\n",
        "    sample = x_test[i][1:1 + int(x_test[i][0])]\n",
        "    mle_shape, mle_scale = gamma_mle(sample)\n",
        "    print(f\"Sample {i+1}: MLE Shape: {mle_shape:.4f}, Predicted Shape: {predicted_shapes[i][0]:.4f}\")\n",
        "    print(f\"Sample {i+1}: MLE Scale: {mle_scale:.4f}, Predicted Scale: {predicted_scales[i][0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVZOAMElOnrf",
        "outputId": "10b86b19-c1e5-4a65-efa2-018cbf47fa5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1022 - shape_output_loss: 0.6366 - scale_output_loss: 0.2090 - shape_output_mean_squared_error: 0.6366 - scale_output_mean_squared_error: 0.2090\n",
            "Test Loss: 1.1021833419799805, Test MSE for Shape: 0.636649489402771, Test MSE for Scale: 0.20896796882152557\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Sample predictions:\n",
            "Predicted Shape (a): 2.7356138229370117, Actual Shape (a): 2.2400439130193637\n",
            "Predicted Scale (b): 1.0634686946868896, Actual Scale (b): 1.0164581573382039\n",
            "Predicted Shape (a): 3.6975035667419434, Actual Shape (a): 4.620680694895188\n",
            "Predicted Scale (b): 1.2403087615966797, Actual Scale (b): 0.7805620624327825\n",
            "Predicted Shape (a): 4.984776496887207, Actual Shape (a): 4.709922154475835\n",
            "Predicted Scale (b): 1.8807728290557861, Actual Scale (b): 1.901226514381733\n",
            "Predicted Shape (a): 2.825852870941162, Actual Shape (a): 2.267330284762958\n",
            "Predicted Scale (b): 1.0725339651107788, Actual Scale (b): 1.049074787546425\n",
            "Predicted Shape (a): 4.049734115600586, Actual Shape (a): 3.8325211006902915\n",
            "Predicted Scale (b): 1.374035358428955, Actual Scale (b): 0.7805620624327825\n",
            "\n",
            "Comparison with MLE:\n",
            "Sample 1: MLE Shape: 2.3750, Predicted Shape: 2.7356\n",
            "Sample 1: MLE Scale: 0.9540, Predicted Scale: 1.0635\n",
            "Sample 2: MLE Shape: 4.0174, Predicted Shape: 3.6975\n",
            "Sample 2: MLE Scale: 0.8957, Predicted Scale: 1.2403\n",
            "Sample 3: MLE Shape: 6.6153, Predicted Shape: 4.9848\n",
            "Sample 3: MLE Scale: 1.3547, Predicted Scale: 1.8808\n",
            "Sample 4: MLE Shape: 3.0409, Predicted Shape: 2.8259\n",
            "Sample 4: MLE Scale: 0.6458, Predicted Scale: 1.0725\n",
            "Sample 5: MLE Shape: 4.3050, Predicted Shape: 4.0497\n",
            "Sample 5: MLE Scale: 0.7298, Predicted Scale: 1.3740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mle_shapes = []\n",
        "mle_scales = []\n",
        "for i in range(len(x_test)):\n",
        "    sample = x_test[i][1:1 + int(x_test[i][0])]\n",
        "    mle_shape, mle_scale = gamma_mle(sample)\n",
        "    mle_shapes.append(mle_shape)\n",
        "    mle_scales.append(mle_scale)\n",
        "\n",
        "mle_shapes = np.array(mle_shapes)\n",
        "mle_scales = np.array(mle_scales)\n",
        "\n",
        "mse_shape_mle = mean_squared_error(y_test_shape, mle_shapes)\n",
        "mse_scale_mle = mean_squared_error(y_test_scale, mle_scales)\n",
        "\n",
        "print(f\"MSE for Shape (a) - MLE: {mse_shape_mle:.4f}, Model: {mse_shape:.4f}\")\n",
        "print(f\"MSE for Scale (b) - MLE: {mse_scale_mle:.4f}, Model: {mse_scale:.4f}\")"
      ],
      "metadata": {
        "id": "AVSyp8ihky5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6466e836-2fa7-4608-963c-d62bfe12c271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Shape (a) - MLE: 1.1726, Model: 0.6366\n",
            "MSE for Scale (b) - MLE: 0.1128, Model: 0.2090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(checkpoint_path):\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print(\"Loaded model weights from checkpoint.path\")"
      ],
      "metadata": {
        "id": "2UNxZ5XeUtXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}